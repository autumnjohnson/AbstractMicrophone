{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/autumnjohnson/AbstractMicrophone/blob/master/June_3_TESTER%20with%20speech.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWQoAI0cXWNH"
      },
      "source": [
        "# VQ-VAE\n",
        "For sperm whales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4K1pcEq2YCbC"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "8df13c49-09a3-47e9-ffca-b218081593dd",
        "id": "xXu5XcXAj045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.6)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.2)\n",
            "Requirement already satisfied: POT in /usr/local/lib/python3.10/dist-packages (0.9.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.12)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.32.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.1->datasets) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install umap-learn datasets POT\n",
        "import numpy as np\n",
        "import io\n",
        "import ot\n",
        "import cv2\n",
        "import torchaudio.transforms as T\n",
        "import ot\n",
        "import scipy\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import EncodecModel, AutoProcessor\n",
        "import umap.umap_ as umap\n",
        "import torchaudio\n",
        "from __future__ import print_function\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from scipy.signal import savgol_filter\n",
        "from six.moves import xrange\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets\n",
        "import torchaudio.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10 as CIFAR10\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import huggingface_hub\n",
        "from huggingface_hub import login\n",
        "from datasets import load_dataset, Audio, Features\n",
        "import pandas\n",
        "import random\n",
        "import matplotlib.cm as cm\n",
        "from librosa import to_mono\n",
        "import requests\n",
        "\n",
        "#from google.colab import userdata\n",
        "import torchaudio\n",
        "from IPython.display import Audio as AudioPlayer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "YFBxYiDXhsqy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he5D8EwFg2UI",
        "outputId": "8f949f18-b5ce-430a-c5fd-ef0bf727ccbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters from authors code\n",
        "batch_size, validation_batch_size, num_training_updates = 64, 32, 15000\n",
        "num_hiddens, num_residual_hiddens, num_residual_layers = 128, 32, 2\n",
        "embedding_dim, num_embeddings = 64, 512\n",
        "commitment_cost, decay, learning_rate = 0.25, 0.99, 1e-4 # decay > 0 uses VQ EMA\n",
        "\n",
        "# Hugging face dataset repo auth\n",
        "repo = \"autumnjohnson/ceti_audio\"\n",
        "token = \"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\"#userdata.get('token')\n",
        "login(token = token)\n",
        "\n",
        "dim = 128\n",
        "\n",
        "speech_params = lambda dim: {\n",
        "     32: {\"hop_length\": 256,  \"n_fft\": 63 },\n",
        "     64: {\"hop_length\": 128,  \"n_fft\": 127 },\n",
        "    128: {\"hop_length\": 32,  \"n_fft\": 255}, # correct\n",
        "    256: {\"hop_length\": 32, \"n_fft\": 511 },\n",
        "    512: {\"hop_length\": 16, \"n_fft\": 1023}\n",
        "    }[dim]\n",
        "whale_params = lambda dim: {\n",
        "     32: {\"hop_length\": 1023,  \"n_fft\": 63 },# correct\n",
        "     64: {\"hop_length\": 512,  \"n_fft\": 127 }, # correct\n",
        "    128: {\"hop_length\": 256,  \"n_fft\": 255}, # correct\n",
        "    256: {\"hop_length\": 128, \"n_fft\": 511 }, # correct\n",
        "    512: {\"hop_length\": 64, \"n_fft\": 1023} # correct\n",
        "    }[dim]\n",
        "\n",
        "params = whale_params(dim)\n",
        "sparams = speech_params(dim)\n",
        "params= sparams\n",
        "\n",
        "cmap = 'RdBu'\n",
        "sr = 16000\n",
        "\n",
        "# Select device\n",
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "2nqzBqyhh6im"
      },
      "outputs": [],
      "source": [
        "def normalize(wav):\n",
        "    return wav / np.abs(wav).max() * 0.999\n",
        "\n",
        "def stft(wav, params):\n",
        "    return np.abs(librosa.stft(y=wav, n_fft=params['n_fft'], hop_length=params['hop_length']))\n",
        "\n",
        "def mfcc(waveform, params):\n",
        "    n_fft, hop_length = params['n_fft'], params['hop_length']\n",
        "    n_mels, n_mfcc, win_length  = 512,  512, None\n",
        "\n",
        "    melspec = librosa.feature.melspectrogram(\n",
        "        y=waveform,\n",
        "        sr=sr,\n",
        "        n_fft=n_fft,\n",
        "        win_length=win_length,\n",
        "        hop_length=hop_length,\n",
        "        n_mels=n_mels,\n",
        "        htk=True,\n",
        "        norm=None)\n",
        "    return librosa.feature.mfcc(\n",
        "        S=librosa.core.spectrum.power_to_db(melspec),\n",
        "        n_mfcc=n_mfcc,\n",
        "        dct_type=2,\n",
        "        sr=sr,\n",
        "        norm=\"ortho\")\n",
        "\n",
        "def rainbow(waveform):\n",
        "    D = librosa.stft(waveform)\n",
        "    freqs = librosa.fft_frequencies()\n",
        "    times = librosa.times_like(D)\n",
        "    mag, phase = librosa.magphase(D)\n",
        "    phase_exp = 2*np.pi*np.multiply.outer(freqs,times)\n",
        "    return  np.diff(np.unwrap(np.angle(phase)-phase_exp, axis=1), axis=1, prepend=0)\n",
        "\n",
        "def get_spectrograms(data, spec_fn, params):\n",
        "    spectrograms = []\n",
        "    for wav in data:\n",
        "        spec = spec_fn(wav, params)\n",
        "        spectrograms.append(spec)\n",
        "    return spectrograms\n",
        "\n",
        "def plot_spectrogram(spectrogram, title = None):\n",
        "    plt.imshow(spectrogram)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def trim(train_data, test_data):\n",
        "    # Get shortest length\n",
        "    all = train_data + test_data\n",
        "    lengths = [len(wav) for wav in all]\n",
        "    min_length = np.min(lengths)\n",
        "    print(\"before trim:\\tmin=\", min_length)\n",
        "\n",
        "    # Trim all to match the length of the shortest audio file\n",
        "    trimmed_train_data = []\n",
        "    for wav in train_data:\n",
        "        diff = (len(wav) - min_length) // 2\n",
        "        trimmed_wav = wav[diff:diff+min_length]\n",
        "        trimmed_train_data.append(trimmed_wav)\n",
        "    trimmed_test_data = []\n",
        "    for wav in test_data:\n",
        "        diff = (len(wav) - min_length) // 2\n",
        "        trimmed_wav = wav[diff:diff+min_length]\n",
        "        trimmed_test_data.append(trimmed_wav)\n",
        "\n",
        "\n",
        "    max_length = np.max([len(wav) for wav in trimmed_train_data])\n",
        "    max_length_test = np.max([len(wav) for wav in trimmed_test_data])\n",
        "    print(f\"after trim:\\tmax train={max_length}, max test={max_length_test}\")\n",
        "    return trimmed_train_data, trimmed_test_data\n",
        "\n",
        "def load_audio(dataset, params):\n",
        "    train_data = dataset['train']['audio']\n",
        "    test_data  = dataset['test']['audio']\n",
        "\n",
        "    train_data = [file['array'] for file in train_data]\n",
        "    test_data  = [file['array'] for file in test_data]\n",
        "\n",
        "    train_trim, test_trim = trim(train_data, test_data)\n",
        "\n",
        "    train_norm = [normalize(y) for y in train_trim]\n",
        "    test_norm  = [normalize(y) for y in test_trim]\n",
        "\n",
        "    train_mu = [librosa.mu_compress(y, mu=255, quantize=False) for y in train_norm]\n",
        "    test_mu  = [librosa.mu_compress(y, mu=255, quantize=False) for y in test_norm]\n",
        "\n",
        "    train_specs = get_spectrograms(train_mu, spec_type, params)\n",
        "    test_specs = get_spectrograms(test_mu, spec_type, params)\n",
        "\n",
        "    train_resize = np.array([librosa.util.frame(item, frame_length=dim, hop_length=params['hop_length']) for item in train_specs])\n",
        "    test_resize  = np.array([librosa.util.frame(item, frame_length=dim, hop_length=params['hop_length']) for item in test_specs])\n",
        "\n",
        "    print(f\"train_resize\\t= {train_resize.shape}\")\n",
        "    print(f\"test_resize\\t= {test_resize.shape}\")\n",
        "\n",
        "    train_data = np.array(train_resize).astype(np.float32).transpose(0, 3, 1, 2)\n",
        "    test_data = np.array(test_resize).astype(np.float32).transpose(0, 3, 1, 2)\n",
        "\n",
        "    train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    test_load  = DataLoader(test_data, batch_size=validation_batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    return train_load, test_load, train_specs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_speech_audio(dataset, params):\n",
        "    train_data = dataset['train']['audio'][:3100]\n",
        "    test_data  = dataset['test']['audio'][:32]\n",
        "\n",
        "    train_data = [file['array'] for file in train_data]\n",
        "    test_data  = [file['array'] for file in test_data]\n",
        "\n",
        "\n",
        "    train_trim, test_trim = trim(train_data, test_data)\n",
        "\n",
        "    train_mu = [librosa.mu_compress(y, mu=255, quantize=False) for y in train_data]\n",
        "    test_mu  = [librosa.mu_compress(y, mu=255, quantize=False) for y in test_data]\n",
        "\n",
        "    train_specs = get_spectrograms(train_mu, spec_type, params)\n",
        "    test_specs = get_spectrograms(test_mu, spec_type, params)\n",
        "\n",
        "    train_resize = np.array([librosa.util.frame(item, frame_length=dim, hop_length=params['hop_length']) for item in train_specs])\n",
        "    test_resize  = np.array([librosa.util.frame(item, frame_length=dim, hop_length=params['hop_length']) for item in test_specs])\n",
        "\n",
        "    print(f\"train_resize\\t= {train_resize.shape}\")\n",
        "    print(f\"test_resize\\t= {test_resize.shape}\")\n",
        "\n",
        "    train_data = np.array(train_resize).astype(np.float32).transpose(0, 3, 1, 2)\n",
        "    test_data = np.array(test_resize).astype(np.float32).transpose(0, 3, 1, 2)\n",
        "\n",
        "    train_load = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "    test_load  = DataLoader(test_data, batch_size=validation_batch_size, shuffle=False, drop_last=False)\n",
        "\n",
        "    return train_load, test_load, train_specs"
      ],
      "metadata": {
        "id": "SCuyWmfusGI2"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spec_type = stft\n",
        "#whale_data = load_dataset(repo).cast_column(\"audio\", Audio(decode=True, sampling_rate=sr))\n",
        "#train_load, test_load, train_specs = load_audio(whale_data, params)\n",
        "#data_variance = np.var(train_load.dataset) / 255.0\n",
        "\n",
        "#print(f\"train_load dataset\\t= {train_load.dataset.shape}\")\n",
        "#print(f\"test_load dataset\\t= {test_load.dataset.shape}\")\n",
        "#print(f\"variance\\t\\t= {data_variance}\")"
      ],
      "metadata": {
        "id": "Uag2FxFk8-K_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "CGxFe9bHZrKN"
      },
      "outputs": [],
      "source": [
        "#y = train_specs[5]\n",
        "#print(f\"spectrogram\\t= {y.shape}\")\n",
        "#y_inv = librosa.griffinlim(y)\n",
        "#librosa.display.specshow(librosa.amplitude_to_db(y), cmap='RdBu',  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "#AudioPlayer(y_inv, rate=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "bJfn03eSTdD1"
      },
      "outputs": [],
      "source": [
        "#ry = train_load.dataset[5][0,:,:]\n",
        "#print(f\"resized spectrogram\\t= {ry.shape}\")\n",
        "#librosa.display.specshow(librosa.amplitude_to_db(ry), cmap='RdBu',  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "#ry_inv = librosa.griffinlim(ry,  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "#AudioPlayer(ry_inv, rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Speech commands dataset"
      ],
      "metadata": {
        "id": "fM2PQga1ha7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sdata = load_dataset(\"google/speech_commands\", \"v0.02\").cast_column(\"audio\", Audio(decode=True, sampling_rate=sr))"
      ],
      "metadata": {
        "id": "xsBuz-ScgsO5"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "strain, stest, sspecs = load_speech_audio(sdata, sparams)\n",
        "svar = np.var(strain.dataset) / 255.0\n",
        "\n",
        "print(f\"train_load dataset\\t= {strain.dataset.shape}\")\n",
        "print(f\"test_load dataset\\t= {stest.dataset.shape}\")\n",
        "print(f\"variance\\t\\t= {svar}\")"
      ],
      "metadata": {
        "id": "XNE-8mlNu8hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = sspecs[5]\n",
        "print(f\"spectrogram\\t= {y.shape}\")\n",
        "y_inv = librosa.griffinlim(y)\n",
        "librosa.display.specshow(librosa.amplitude_to_db(y), cmap='RdBu',  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "AudioPlayer(y_inv, rate=sr)"
      ],
      "metadata": {
        "id": "Q3jEH8oEkHzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ry = strain.dataset[5][0,:,:]\n",
        "print(f\"resized spectrogram\\t= {ry.shape}\")\n",
        "librosa.display.specshow(librosa.amplitude_to_db(ry), cmap='RdBu',  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "ry_inv = librosa.griffinlim(ry,  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "AudioPlayer(ry_inv, rate=sr)"
      ],
      "metadata": {
        "id": "RJnJYLoukIBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvTQqEG_qX-G"
      },
      "source": [
        "# Vector quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfrPgnRhMQOf"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizer(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
        "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
        "\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XB50wyGJsKl-"
      },
      "outputs": [],
      "source": [
        "class VectorQuantizerEMA(nn.Module):\n",
        "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):\n",
        "        super(VectorQuantizerEMA, self).__init__()\n",
        "\n",
        "        self._embedding_dim = embedding_dim\n",
        "        self._num_embeddings = num_embeddings\n",
        "\n",
        "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
        "        self._embedding.weight.data.normal_()\n",
        "        self._commitment_cost = commitment_cost\n",
        "\n",
        "        self.register_buffer('_ema_cluster_size', torch.zeros(num_embeddings))\n",
        "        self._ema_w = nn.Parameter(torch.Tensor(num_embeddings, self._embedding_dim))\n",
        "        self._ema_w.data.normal_()\n",
        "\n",
        "        self._decay = decay\n",
        "        self._epsilon = epsilon\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # convert inputs from BCHW -> BHWC\n",
        "        inputs = inputs.permute(0, 2, 3, 1).contiguous()\n",
        "        input_shape = inputs.shape\n",
        "\n",
        "        # Flatten input\n",
        "        flat_input = inputs.view(-1, self._embedding_dim)\n",
        "\n",
        "        # Calculate distances\n",
        "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)\n",
        "                    + torch.sum(self._embedding.weight**2, dim=1)\n",
        "                    - 2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
        "\n",
        "        # Encoding\n",
        "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
        "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
        "        encodings.scatter_(1, encoding_indices, 1)\n",
        "\n",
        "        # Quantize and unflatten\n",
        "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
        "\n",
        "        # Use EMA to update the embedding vectors\n",
        "        if self.training:\n",
        "            self._ema_cluster_size = self._ema_cluster_size * self._decay + \\\n",
        "                                     (1 - self._decay) * torch.sum(encodings, 0)\n",
        "\n",
        "            # Laplace smoothing of the cluster size\n",
        "            n = torch.sum(self._ema_cluster_size.data)\n",
        "            self._ema_cluster_size = (\n",
        "                (self._ema_cluster_size + self._epsilon)\n",
        "                / (n + self._num_embeddings * self._epsilon) * n)\n",
        "\n",
        "            dw = torch.matmul(encodings.t(), flat_input)\n",
        "            self._ema_w = nn.Parameter(self._ema_w * self._decay + (1 - self._decay) * dw)\n",
        "\n",
        "            self._embedding.weight = nn.Parameter(self._ema_w / self._ema_cluster_size.unsqueeze(1))\n",
        "\n",
        "        # Loss\n",
        "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
        "        loss = self._commitment_cost * e_latent_loss\n",
        "\n",
        "        # Straight Through Estimator\n",
        "        quantized = inputs + (quantized - inputs).detach()\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        # convert quantized from BHWC -> BCHW\n",
        "        return loss, quantized.permute(0, 3, 1, 2).contiguous(), perplexity, encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmd_SXKiqhap"
      },
      "source": [
        "# Model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9Hodr_4uJBs"
      },
      "outputs": [],
      "source": [
        "class Residual(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_hiddens):\n",
        "        super(Residual, self).__init__()\n",
        "        self._block = nn.Sequential(\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=in_channels,\n",
        "                      out_channels=num_residual_hiddens,\n",
        "                      kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels=num_residual_hiddens,\n",
        "                      out_channels=num_hiddens,\n",
        "                      kernel_size=1, stride=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self._block(x)\n",
        "\n",
        "class ResidualStack(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(ResidualStack, self).__init__()\n",
        "        self._num_residual_layers = num_residual_layers\n",
        "        self._layers = nn.ModuleList([Residual(in_channels, num_hiddens, num_residual_hiddens)\n",
        "                             for _ in range(self._num_residual_layers)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self._num_residual_layers):\n",
        "            x = self._layers[i](x)\n",
        "        return F.relu(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H57fmloSsZjs"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1, padding=1)\n",
        "\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "        self._conv_trans_1 = nn.ConvTranspose2d(in_channels=num_hiddens,\n",
        "                                                out_channels=num_hiddens//2,\n",
        "                                                kernel_size=4,\n",
        "                                                stride=2, padding=1)\n",
        "\n",
        "        self._conv_trans_2 = nn.ConvTranspose2d(in_channels=num_hiddens//2,\n",
        "                                                out_channels=1,\n",
        "                                                kernel_size=4,\n",
        "                                                stride=2, padding=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "\n",
        "        x = self._residual_stack(x)\n",
        "\n",
        "        x = self._conv_trans_1(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        return self._conv_trans_2(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5F7v5BPscIV"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self._conv_1 = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=num_hiddens//2,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "        self._conv_2 = nn.Conv2d(in_channels=num_hiddens//2,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=4,\n",
        "                                 stride=2, padding=1)\n",
        "        self._conv_3 = nn.Conv2d(in_channels=num_hiddens,\n",
        "                                 out_channels=num_hiddens,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1, padding=1)\n",
        "        self._residual_stack = ResidualStack(in_channels=num_hiddens,\n",
        "                                             num_hiddens=num_hiddens,\n",
        "                                             num_residual_layers=num_residual_layers,\n",
        "                                             num_residual_hiddens=num_residual_hiddens)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self._conv_1(inputs)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._conv_2(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        x = self._conv_3(x)\n",
        "        return self._residual_stack(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeM7ofCGquGH"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "                 num_embeddings, embedding_dim, commitment_cost, decay=0):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        self._encoder = Encoder(1, num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "        self._pre_vq_conv = nn.Conv2d(in_channels=num_hiddens,\n",
        "                                      out_channels=embedding_dim,\n",
        "                                      kernel_size=1,\n",
        "                                      stride=1)\n",
        "        if decay > 0.0:\n",
        "            self._vq_vae = VectorQuantizerEMA(num_embeddings, embedding_dim,\n",
        "                                              commitment_cost, decay)\n",
        "        else:\n",
        "            self._vq_vae = VectorQuantizer(num_embeddings, embedding_dim,\n",
        "                                           commitment_cost)\n",
        "        self._decoder = Decoder(embedding_dim,\n",
        "                                num_hiddens,\n",
        "                                num_residual_layers,\n",
        "                                num_residual_hiddens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self._encoder(x)\n",
        "        z = self._pre_vq_conv(z)\n",
        "        loss, quantized, perplexity, _ = self._vq_vae(z)\n",
        "        x_recon = self._decoder(quantized)\n",
        "\n",
        "        return loss, x_recon, perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_SQJe6nwTO"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wzu66GDsSop"
      },
      "outputs": [],
      "source": [
        "def train(model, data_load, optimizer, epoch):\n",
        "    # Run training loop\n",
        "    model.train()\n",
        "\n",
        "    train_res_recon_error = []\n",
        "    train_res_perplexity = []\n",
        "    train_res_loss = []\n",
        "    print(\"Training...\", data_load.dataset.shape)\n",
        "\n",
        "    for i in xrange(num_training_updates):\n",
        "        data = next(iter(data_load))\n",
        "        data = data.to(device).float()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        vq_loss, data_recon, perplexity = model(data)\n",
        "        recon_error = F.mse_loss(data_recon, data) / data_variance\n",
        "        loss = recon_error + vq_loss\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        train_res_recon_error.append(recon_error.item())\n",
        "        train_res_perplexity.append(perplexity.item())\n",
        "        train_res_loss.append(loss.item())\n",
        "\n",
        "        if (i+1) % epoch == 0:\n",
        "            print('%d iterations' % (i+1))\n",
        "            print('loss: %.3f' % np.mean(train_res_loss[-epoch:]))\n",
        "            print('recon_error: %.3f' % np.mean(train_res_recon_error[-epoch:]))\n",
        "            print('perplexity: %.3f' % np.mean(train_res_perplexity[-epoch:]))\n",
        "            print()\n",
        "    return train_res_recon_error, train_res_perplexity, train_res_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94fF_BrKJakF"
      },
      "outputs": [],
      "source": [
        "# Create optimizer and model\n",
        "model = Model(num_hiddens, num_residual_layers, num_residual_hiddens,\n",
        "              num_embeddings, embedding_dim,\n",
        "              commitment_cost, decay).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
        "train_res_recon_error, train_res_perplexity, train_res_loss = train(model, train_load, optimizer, 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvEHIibeh5o9"
      },
      "source": [
        "# Plot loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_vae_training_plot(train_losses, test_losses, title, fname):\n",
        "    elbo_train, recon_train, kl_train = train_losses[:, 0], train_losses[:, 1], train_losses[:, 2]\n",
        "    elbo_test, recon_test, kl_test = test_losses[:, 0], test_losses[:, 1], test_losses[:, 2]\n",
        "    plt.figure()\n",
        "    n_epochs = len(test_losses) - 1\n",
        "    x_train = np.linspace(0, n_epochs, len(train_losses))\n",
        "    x_test = np.arange(n_epochs + 1)\n",
        "\n",
        "    plt.plot(x_train, elbo_train, label='-elbo_train')\n",
        "    plt.plot(x_train, recon_train, label='recon_loss_train')\n",
        "    plt.plot(x_train, kl_train, label='kl_loss_train')\n",
        "    plt.plot(x_test, elbo_test, label='-elbo_test')\n",
        "    plt.plot(x_test, recon_test, label='recon_loss_test')\n",
        "    plt.plot(x_test, kl_test, label='kl_loss_test')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    savefig(fname)"
      ],
      "metadata": {
        "id": "9kW6ue112OXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb8wDr8TNSWN"
      },
      "outputs": [],
      "source": [
        "# Run the code in this cell to define the variable 'train_res_recon_error_smooth'\n",
        "train_res_recon_error_smooth = savgol_filter(train_res_recon_error, 201, 7)\n",
        "train_res_perplexity_smooth = savgol_filter(train_res_perplexity, 201, 7)\n",
        "train_res_loss_smooth = savgol_filter(train_res_loss, 201, 7)\n",
        "\n",
        "# Then run the code in this cell to plot the data\n",
        "f = plt.figure(figsize=(8,3))\n",
        "ax = f.add_subplot(1,2,1)\n",
        "ax.plot(train_res_recon_error_smooth)\n",
        "ax.set_yscale('log')\n",
        "ax.set_title('NMSE') # Smoothed NMSE\n",
        "ax.set_xlabel('iteration')\n",
        "\n",
        "ax = f.add_subplot(1,2,2)\n",
        "ax.plot(train_res_perplexity_smooth)\n",
        "ax.set_title('Perplexity') #Smoothed Average codebook usage (perplexity)\n",
        "ax.set_xlabel('iteration')\n",
        "\n",
        "f = plt.figure(figsize=(8,3))\n",
        "ax = f.add_subplot(1,2,2)\n",
        "ax.plot(train_res_loss_smooth)\n",
        "ax.set_title('Loss') # Smoothed loss\n",
        "ax.set_xlabel('iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ4VIPPIWyl-"
      },
      "source": [
        "# View reconstructions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def show_grid(img, title = None):\n",
        "    img = librosa.display.specshow(img, hop_length=params['hop_length'], n_fft=params['n_fft'], cmap='RdBu', sr=sr).get_figure()\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    plt.cmap = 'RdBu'\n",
        "\n",
        "    plt.show(img)\n"
      ],
      "metadata": {
        "id": "kc0YrtAZ1K_x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsiQVMZuJakG"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "valid_originals = next(iter(test_load))\n",
        "valid_originals = valid_originals.to(device)\n",
        "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
        "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
        "valid_reconstructions = (model._decoder(valid_quantize)).detach().cpu()\n",
        "\n",
        "train_originals  = next(iter(train_load))\n",
        "train_originals = train_originals.to(device)\n",
        "_, train_reconstructions, _, _ = model._vq_vae(train_originals)\n",
        "\n",
        "oimgs = valid_originals.detach().cpu()\n",
        "rimgs = valid_reconstructions.detach().cpu()\n",
        "qimgs = valid_quantize.detach().cpu().numpy()\n",
        "print(f\"qimgs\\t= {qimgs.shape}\")\n",
        "print(f\"rimgs\\t= {rimgs.shape}\")\n",
        "print(f\"oimgs\\t= {oimgs.shape}\")\n",
        "\n",
        "ogrid = make_grid(torch.tensor(librosa.amplitude_to_db(oimgs)), normalize=True) # 32 x 1 x 256 x 256\n",
        "rgrid = make_grid(torch.tensor(librosa.amplitude_to_db(rimgs)), normalize=True)\n",
        "\n",
        "show_grid(librosa.amplitude_to_db(oimgs[10][0]), title=\"Original\")\n",
        "show_grid(librosa.amplitude_to_db(qimgs[10][0]), title=\"Quantized\")\n",
        "show_grid(librosa.amplitude_to_db(rimgs[10][0]), title=\"Reconstruction\")\n",
        "\n",
        "show_grid(ogrid[0].numpy(), title=\"Originals\")\n",
        "show_grid(rgrid[0].numpy(), title=\"Reconstructions\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmn5SCtfLOuF"
      },
      "outputs": [],
      "source": [
        "recon = valid_reconstructions[1][0].numpy()\n",
        "recon2 = valid_reconstructions[2][0].numpy()\n",
        "recon3 = valid_reconstructions[3][0].numpy()\n",
        "\n",
        "y_inv = librosa.griffinlim(recon,  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "y_inv2 = librosa.griffinlim(recon2,  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "y_inv3 = librosa.griffinlim(recon3,  hop_length=params['hop_length'], n_fft=params['n_fft'])\n",
        "\n",
        "display(AudioPlayer(y_inv, rate=sr))\n",
        "display(AudioPlayer(y_inv2, rate=sr))\n",
        "display(AudioPlayer(y_inv3, rate=sr))\n",
        "\n",
        "display(librosa.display.specshow(librosa.amplitude_to_db(recon),  hop_length=params['hop_length'], n_fft=params['n_fft'], cmap='RdBu', sr=sr).get_figure())\n",
        "display(librosa.display.specshow(librosa.amplitude_to_db(recon2),  hop_length=params['hop_length'], n_fft=params['n_fft'], cmap='RdBu', sr=sr).get_figure())\n",
        "librosa.display.specshow(librosa.amplitude_to_db(recon3),  hop_length=params['hop_length'], n_fft=params['n_fft'], cmap='RdBu', sr=sr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nCS1OfiW3yA"
      },
      "outputs": [],
      "source": [
        "# Get embedding data - assumes model _vq_vae._embedding.weight stores embeddings\n",
        "whale_embed = model._vq_vae._embedding.weight.data.detach().cpu().numpy()\n",
        "show_grid(whale_embed, f\"Embeddings [{embedding_dim}, {num_embeddings}]\")\n",
        "plot_spectrogram(whale_embed, f\"Embeddings [{embedding_dim}, {num_embeddings}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjhpi73xWRlD"
      },
      "outputs": [],
      "source": [
        "proj = umap.UMAP(n_neighbors=3,min_dist=0.1, metric='cosine').fit_transform(whale_embed)\n",
        "plt.title(\"Embeddings (umap)\")\n",
        "plt.scatter(proj[:,0], proj[:,1], alpha=0.3)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "collapsed_sections": [
        "nvTQqEG_qX-G",
        "kmd_SXKiqhap"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}